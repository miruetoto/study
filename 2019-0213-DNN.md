---
layout: post
title: (정리) DNN
---

### About this document

- 딥러닝기초

- 학부수준 

- 이 포스트는 DNN에 대하여 다룬다.

### DNN 아키텍처

- 일반적으로 교재에 설명되어 있는 DNN아키텍쳐의 경우 하나의 observation만 고려한다. 왜냐하면 아키텍쳐는 observation별로 모두 동일하기 때문이다. 즉 하나의 observation에 대한 아키텍처만 설계하면 다른 observation에 대한 아키텍쳐도 동일하게 **copy and paste** 로 설계하면된다.

- $i$번째 관측치의 DNN은 대략 아래와 같은 느낌의 구조를 가지고 있다.
\begin{align}
{\bf y}_ i = f_2\big(f_1({\bf x}_ i {\bf W_1}+{\bf b_1}){\bf W_2}+{\bf b_2}\big){\bf W_3}+{\bf b_3} + {\boldsymbol\epsilon}_ i
\end{align}
각 요소들의 디멘젼을 **굳이 따지면** 아래와 같이 정리할 수 있다(이런거 까지 다 써야하나 싶긴하지만 처음 한번은 디멘젼을 따져보는게 좋다).

<center><style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top}
.tg .tg-7btt{font-weight:bold;border-color:inherit;text-align:center;vertical-align:top}
.tg .tg-fymr{font-weight:bold;border-color:inherit;text-align:left;vertical-align:top}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
</style>
<table class="tg">
  <tr>
    <td class="tg-0pky">${\bf x}_ i$</td>
    <td class="tg-0pky">$1\times p$ row-vector</td>
  </tr>
  <tr>
    <td class="tg-0pky">${\bf W_1}$</td>
    <td class="tg-0pky">$p\times p_1$ matrix</td>
  </tr>
  <tr>
    <td class="tg-0pky">${\bf b_1}$</td>
    <td class="tg-0pky">$1\times p_1$ row-vector</td>
  </tr>
  <tr>
    <td class="tg-0pky">$f_1$</td>
    <td class="tg-0pky">$\mathbb{R}^{1 \times p_1} \rightarrow \mathbb{R}^{1 \times p_1}$ vector function</td>
  </tr>
  <tr>
    <td class="tg-0pky">${\bf W_2}$</td>
    <td class="tg-0pky">$p_1\times p_2$ matrix</td>
  </tr>
  <tr>
    <td class="tg-0pky">${\bf b_2}$</td>
    <td class="tg-0pky">$1\times p_2$ row-vector</td>
  </tr>
  <tr>
    <td class="tg-0pky">$f_2$</td>
    <td class="tg-0pky">$\mathbb{R}^{1 \times p_2} \rightarrow \mathbb{R}^{1 \times p_2}$ vector function</td>
  </tr>
  <tr>
    <td class="tg-0pky">${\bf W_3}$</td>
    <td class="tg-0pky">$p_2\times p_3$ matrix</td>
  </tr>
  <tr>
    <td class="tg-0pky">${\bf b_3}$</td>
    <td class="tg-0pky">$1\times p_3$ row-vector</td>
  </tr>
  <tr>
    <td class="tg-0pky">${\bf y}_i$</td>
    <td class="tg-0pky">$1\times p_3$ row-vector</td>
  </tr>
  <tr>
    <td class="tg-0pky">${\boldsymbol\epsilon}_ i$</td>
    <td class="tg-0pky">$1\times p_3$ row-vector</td>
  </tr>  
</table></center>

- 잘 살펴보면 입력데이터 ${\bf x}_ i$를 어떠한 변환 ${\bf W}_ 1$를 활용하여 선형변환하고 그 뒤에 비선형함수 $f_1$를 씌우는 것을 반복하는 형태이다. 진짜 별 내용이 없다.

- 여러개의 observation의 경우에는 아래와 같이 간단하게 나타낼 수 있다.
\begin{align}
{\bf y} = f_2\big(f_1({\bf X} {\bf W_1}+{\bf b_1}){\bf W_2}+{\bf b_2}\big){\bf W_3}+{\bf b_3} + {\boldsymbol \epsilon}
\end{align}
매우 간결하다. 하지만 사실 이건 거의 나만 쓰는 노테이션이다. 책에서는 보통 저렇게 표현안하고 하나의 observation만 주로 표현하는데 그 이유는 저 노테이션이 혼동의 여지가 다분하기 때문이다(하지만 나는 간결한게 좋기 때문에 그냥 위와 같이 쓸란다). 당장 ${\bf b_1}$만 보아도 $i$-th observation만 고려하는 식에서는 디멘젼이 $1\times p_1$ row-vector이지만 모든 observation을 고려하는 식에서는 $n\times p_1$이 된다. 그 외에도 ${\bf y}$가 $n\times 1$ 벡터가 아니라 $n\times p_3$ 매트릭스라서 원래는 ${\bf Y}$로 쓰는게 더 맞을것 같지만 위에는 관습적으로 ${\bf y}$를 썼다는 둥의 사소한 문제가 있다(왠지 이렇게 써야지 종속변수 같은 느낌이 더 들어서 나는 저렇게 쓴다. 또한 나중에 손실함수 미분하면서 알겠지만 어차피 WLOG ${\bf y}$ 를 $n\times 1$ vector로 생각할 수 있다는 사실을 알게 된다). 마지막으로 가장 심각하게 헷갈리는건 $f$가 $\mathbb{R}^{n\times p_2} \rightarrow \mathbb{R}^{n \times p_2}$와 같이 matrix 를 matrix 로 맵핑하는 함수처럼 보이는데 실제로는 row-vector 를 row-vector 로 맵핑하는 형태의 함수이다. 이건 당연한게 observation 마다 다른 활성화 함수를 적용할 수 는 없는 노릇이니 **each row** 에 적용되는 $f$는 동일할 수밖에 없다. 그리고 사실 엄밀하게 말하면 row-vector 를 row-vector로 맵핑하는 경우도 거의 없고 (소프트맥스 정도만 여기에 해당함) 아래와 같이 scalar to scalar mapping 인 경우가 대부분이다. 주로 사용되는 $f$는 아래와 같다. <br/><br/>
**(1)** $f$가 항등함수인 경우:
\begin{align}
f ~ \big(({\bf X W+b})_ {ij}\big)=({\bf X W+b})_ {ij}
\end{align}
<br/>
**(2)** $f$가 로지스틱함수(=시그모이드함수)인 경우:
\begin{align}
f~ \big(({\bf X W+b})_ {ij}\big)=\frac{e^{({\bf X W+b})_ {ij}}}{1+e^{({\bf X W+b})_ {ij}}}=\frac{1}{1+e^{-({\bf X W+b})_ {ij}}}
\end{align}
<br/>
**(3)** $f$가 소프트맥스함수인 경우:
\begin{align}
f \big({\bf x}_ i {\bf W}+{\bf b}\big) = \frac{e^{ {\bf x}_ i {\bf W}+{\bf b} } }{e^{ {\bf x}_ i {\bf W}+{\bf b} }{\bf 1}}
\end{align}
<br/>
소프트맥스 함수에서 $\exp$와 $/$는 **elementwise operation** 이다. 따라서 위에서 $e^{ {\bf x}_ i {\bf W}+{\bf b} }$는 row-vector가 된다(왜냐하면 ${\bf x}_ i {\bf W}+{\bf b}$ 가 row-vector 이니까). 따라서 분자는 row-vector 이고 분모는 row-vector $\times$ col-vector 꼴이니까 scalar가 된다. 그래서 $f \big({\bf x}_ i {\bf W}+{\bf b} \big)$ 의 차원은 row-vector 가 된다(이런 당연한 것도 첨에는 한번씩 다 따져보는게 좋다). 즉 소프트맥스의 경우 입력이 row-vector ${\bf x}_ i {\bf W}+{\bf b} $이고, 출력 $f \big({\bf x}_ i {\bf W}+{\bf b} \big)$ 역시 row-vector 이므로 row-vector 를 row-vector 로 맵핑하는 함수가 된다.

- 참고로 각층의 입력에 $\bf 1$ 벡터를 추가하면 바이어스텀을 생략할 수 있다. 앞으로는 그냥 바이어스텀을 생략하고 쓰겠다. 따라서 모델은 아래와 같이 된다.
\begin{align}
{\bf y} = f_2\big(f_1({\bf X} {\bf W_1}){\bf W_2} \big){\bf W_3} + {\boldsymbol \epsilon}
\end{align}

### 기울기계산

- 신경망을 아래와 같이 쓸 수 있다.
\begin{align}
{\bf y} = f_2\big(f_1({\bf X} {\bf W_1}){\bf W_2}\big){\bf W_3} + {\boldsymbol \epsilon}
\end{align}
우리의 목적은
\begin{align}
J=tr\Big(\big({\bf y}-{\bf\hat y}\big)'\big({\bf y}-{\bf\hat y}\big)\Big)
\end{align}
을 최소화하는 적당한 $\bf W_1, W_2, W_3$을 찾는 것이다. 주의할 점은
\begin{align}
\big({\bf y}-{\bf\hat y}\big)'\big({\bf y}-{\bf\hat y}\big)
\end{align}
은 $p_3\times p_3$ 매트릭스 이지만 $J$는 스칼라라는 점이다. 트레이스는 선형함수이므로 앞으로 진행되는 논의에서 ${\bf y}$ 는 $n \times 1$ vector 라고 생각해도 무방하다(앞으로 미분만 할건데, 더해진것을 미분하는것은 각각 미분해서 더한것과 같으므로 요런식의 WLOG가 가능하다). 즉 $p_3=1$ 로 두어도 무방하다는 의미이다. 따라서 앞으로는 ${\bf y}$ 는 그냥 $n \times 1$ vector 라고 가정하겠다.

- 따라서 아래를 연립해서 풀면된다.
\begin{align}
\begin{cases}
\frac{\partial J}{\partial {\bf W_1} }=0 \\\\ \\
\frac{\partial J}{\partial {\bf W_2} }=0 \\\\ \\
\frac{\partial J}{\partial {\bf W_3} }=0
\end{cases}
\end{align}

- 편의상 아래와 같이 기호를 정의하자.
\begin{align}
\begin{cases}
{\bf \tilde X_3} := g\big(f({\bf X} {\bf W_1}){\bf W_2} \big) \\\\ \\
{\bf \tilde X_2} := f({\bf X} {\bf W_1}) \\\\ \\
{\bf \tilde X_1} := {\bf X}
\end{cases}
\end{align}
그러면 아래와 같이 쓸 수 있다
\begin{align}
{\bf y} & = f_2\big(f_1({\bf X} {\bf W_1}){\bf W_2}\big){\bf W_3} + {\boldsymbol \epsilon}  \\\\ \\
& = f_2\big(f_1({\bf \tilde X_1} {\bf W_1}){\bf W_2}\big){\bf W_3} + {\boldsymbol \epsilon} \\\\ \\
& = f_2({\bf \tilde X_2}{\bf W_2}){\bf W_3} + {\boldsymbol \epsilon} \\\\ \\
& = {\bf \tilde X_3}{\bf W_3} + {\boldsymbol \epsilon}
\end{align}

- 이제 하나씩 풀어보자.
\begin{align}
J={\boldsymbol\epsilon}'{\boldsymbol\epsilon}
=\big({\bf y}-{\bf \tilde X_3}{\bf W_3}\big)'\big({\bf y}-{\bf \tilde X_3}{\bf W_3}\big)
\end{align}
따라서 $\frac{\partial J}{\partial \bf W_3}$을 계산하면 아래와 같이 된다.
\begin{align}
\frac{\partial J}{\partial \bf W_3}=-2{\bf \tilde X_3'}{\bf y} + 2 {\bf \tilde X_3'}{\bf \tilde X_3}{\bf W_3}
\end{align}
따라서 $\frac{\partial J}{\partial \bf W_3}=0$을 풀면
\begin{align}
{\bf \hat W_3}=\big( {\bf \tilde X_3'}{\bf \tilde X_3}\big)^{-1}{\bf \tilde X_3'} {\bf y}
\end{align}
이다.

- 이제 $\frac{\partial J}{\partial \bf W_2}$ 를 계산하여 보자.  
\begin{align}
\frac{\partial J}{\partial {\bf W_2}}=\frac{\partial J}{\partial {\bf \tilde X_3'} }\frac{\partial {\bf \tilde X_3'}}{\partial {\bf W_2} }
\end{align}
참고로 ${\bf \tilde X_3'}$ 는 $p_2 \times n$ 이고 ${\bf W_2}$는 $p_1 \times p_2$ 이므로 아래의 미분은 잘 정의된다(요 차원을 맞출려고 $\bf \tilde X_3$가 아닌 $\bf \tilde X_3'$로 미분하는거).
\begin{align}
\frac{\partial {\bf \tilde X_3'} }{\partial {\bf W_2} } .
\end{align}
이제 $\frac{\partial J}{\partial {\bf \tilde X_3'} },~ \frac{\partial {\bf \tilde X_3'}}{\partial {\bf W_2} }$ 를 각각 계산하여보자. 먼저
\begin{align}
\frac{\partial J}{\partial {\bf \tilde X_3'} }=-2{\bf y}'{\bf W_3}+2{\bf W_3'}{\bf \tilde X_3'}{\bf W_3}
\end{align}
이 된다. 그런데 여기에서 ${\bf \tilde X_3}{\bf \hat W_3}={\bf \hat y}$ 임을 이용하면 아래와 같이 예쁘게 쓸 수 있다.
\begin{align}
\frac{\partial J}{\partial {\bf \tilde X_3'} }=2\big({\bf \hat y}-{\bf y})'{\bf W_3}
\end{align}
종종 교재에서 위와같은식으로 표현을 많이하지만 굳이 저렇게 예쁘게 쓸 필요는 없어보인다(개인적인 생각임). 
이제 $\frac{\partial {\bf \tilde X_3'}}{\partial {\bf W_2} }$를 계산하여보자.
\begin{align}
\frac{\partial {\bf \tilde X_3'}}{\partial {\bf W_2} }
=\frac{\partial f_2({\bf W_2'}{\bf \tilde X_2'}) }{\partial {\bf W_2} }
=\frac{\partial f_2({\bf W_2'}{\bf \tilde X_2'}) }{\partial {\bf W_2'}{\bf \tilde X_2'}} \frac{\partial {\bf W_2'}{\bf \tilde X_2'} }{\partial {\bf W_2} } = \dot f_2({\bf W_2'}{\bf \tilde X_2'}){\bf \tilde X_2'}
\end{align}
여기에서 $\dot f_2$ 는 $f_2$의 도함수를 의미한다. 따라서 아래와 같이 정리된다.
\begin{align}
\frac{\partial J}{\partial \bf W_2}=2\big({\bf \hat y}-{\bf y})'{\bf W_3}\dot f_2({\bf W_2'}{\bf \tilde X_2'}){\bf \tilde X_2'}
\end{align}

- 이제 $\frac{\partial J}{\partial \bf W_1}$ 를 계산하여 보자. 아래와 같이 쓸 수 있다.
\begin{align}
\frac{\partial J}{\partial {\bf W_1}}=\frac{\partial J}{\partial {\bf \tilde X_2'} }\frac{\partial {\bf \tilde X_2'}}{\partial {\bf W_1} }=\frac{\partial J}{\partial {\bf \tilde X_3'} }\frac{\partial {\bf \tilde X_3'}}{\partial {\bf \tilde X_2'} }\frac{\partial {\bf \tilde X_2'}}{\partial {\bf W_1} }.
\end{align}
여기에서
\begin{align}
\frac{\partial {\bf \tilde X_3'}}{\partial {\bf \tilde X_2'} }=\dot{f}_ 2({\bf W_2'}{\bf \tilde X_2'}){\bf W_2}
\end{align}
이고
\begin{align}
\frac{\partial {\bf \tilde X_2'}}{\partial {\bf W_1} }= \dot f_1({\bf W_1'}{\bf \tilde X_1'}){\bf \tilde X_1'}
\end{align}
이다. 따라서 그대로 대입하면 쉽게 원하는걸 얻을 수 있다.

- 층이 더 깊은 신경망에서도 위와같은 방법으로 계산할 수 있다. 가령 층이 $L$개라고 한다면 
\begin{align}
\frac{\partial J}{\partial {\bf W_1} }=\frac{\partial J}{\partial {\bf \tilde X_L'} }\frac{\partial {\bf \tilde X_L'} }{\partial {\bf \tilde X_{L-1}'} }\dots \frac{\partial {\bf \tilde X_2'} }{\partial {\bf \tilde X_1'} } \frac{\partial {\bf \tilde X_1'} }{\partial {\bf W_1} }
\end{align}
와 같이 나타낼수 있다. 그런데 임의의 $\ell \in \\{1, \dots, L\\}$ 에 대하여 
\begin{align}
\frac{\partial {\bf \tilde X_{\ell +1}'}}{\partial {\bf \tilde X_{\ell}'} }=\dot{f}_ {\ell}({\bf W_{\ell}'}{\bf \tilde X_{\ell}'}){\bf W_{\ell} }
\end{align}
와 같이 표현할 수 있으므로, $\frac{\partial J}{\partial {\bf W_1} }$을 쉽게 계산할 수 있다. 

