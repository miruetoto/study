---
title: (정리) MLE의 점근적 성질 
layout: post
---

### About this doc

- 통계공부

- 학부수준 혹은 대학원 1-2학년수준 

- 이번 포스팅에서는 MLE의 점근적 성질에 대하여 다룬다. 

- 참고한 교재는 김우철의 수리통계학, 호그 5판이다. 

- 또한 대학원 통계이론 강의노트도 참고하였다. (MCE와 비교하기위함)


### MME를 왜 안 쓰는가? 

- 보통 MLE를 설명하기 전에 적률추정량(method of moments estimator, MME)이란걸 배운다. 적률추정량이란 모집단의 적률의 추정할때 표본적률을 사용한다는 단순한 추정량이다. 이를 이용하면 우리가 관심있는 모수를 적률의 함수로 표현하기만 하면(내 생각엔 앵간한 모수는 거의 다 적률로 표현할 수는 있을 것 같다) 바로 추정량을 얻을 수 있다. 가령 우리가 어떤 확률변수 $X$의 평균과 분산에 관심있다고 하자. 그런데 평균은 일차적률 그 자체이고 분산은 이차적률에서 일차적률제곱을 뺀 형태로 표현가능하다. 즉 
\begin{align}
V(X)=EX^2-(EX)^2 
\end{align}
와 같이 표현할 수 있다. 따라서 적률추정법을 이용하면 평균과 분산에 대한 estimator를 아래와 같이 구할 수 있다. 
\begin{align}
\mbox{estimator of }EX=&\bar{X} \\\\ \\
\mbox{estimator of }V(X)=&\frac{1}{n}\sum_{i=1}^{n}X^2-\bar{X}^2.
\end{align}
MME는 정말 구하기 쉽다. MLE를 구하기 위해서는 우도함수를 구해서 미분을 하다보면 MME가 얼마나 쉽게 얻어질 수 있는지 체감될 것이다. 

- 적률추정량의 또 다른 장점은 LLN(law of large number)덕에 일치성을 보이기가 매우 쉽다는 것이다. 예를 들어 분산의 MME의 경우 $E\|X^2\|<\infty$임을 가정하기만 하면 쉽게 
\begin{align}
\frac{1}{n}\sum_{i=1}^{n}X^2-\bar{X}^2 \quad \overset{p}{\operatorname \rightarrow} \quad V(X)
\end{align}
임을 보일 수 있다. 이따가 보일 MLE의 일치성을 증명하는 과정을 따라가다 보면 이것이 얼마나 사기적인 장점인지 체감할 것이다. 

- 이러한 장점들에도 불구하고 (엄청 쉽게 estimator를 얻을 수 있는데, 심지어 얻어진 estimator가 거의 반자동으로 일치추정량임) MME를 쓰지 않는 이유는 무엇일까? 처음에는 MME가 유일하지 않기 때문이라 생각했다. (왜 MME가 유일하지 않은지 모르겠다면 포아송분포를 생각해보자.) 그런데 MLE도 유일하지 않은 경우가 있다. (김우철 수리통계 p.265) 그래서 나름대로 MLE를 쓰는 이유를 생각해봤는데 다음과 같은 이유들이 있을것 같다. **(1)** 점근적 성질에 대한 이론적 토대가 탄탄하다. (피셔의 정보량등..) **(2)** LSE, 베이즈추정량 같은 estimator도 MLE와 밀접한 연관이 있다. **(3)** EM-알고리즘등 매우 유용한 응용들이 MLE를 중심으로 개발되어 있다(즉 MLE가 이론적으로 탄탄해서 이를 활용한 다양한 방법을 개발하기 쉬운모양). 

- 이런 이유로 이제 MME는 손절하고 MLE로 넘어가자. 

### MLE의 일치성 

- MLE의 일치성을 만족하기 위해서 어떠한 조건이 필요한지 직관적으로 생각해보자. 우선 아래와 같은 함수를 가정하자. 
\begin{align}
l(\theta; {\boldsymbol X}):=l(\theta; X_1,\dots,X_n)=\sum_{i=1}^{n}\log f(X_i;\theta)
\end{align}
일반적인 likelihood와 다르게 이것은 관측치 $x_1,\dots,x_n$가 아니라 랜덤표본 $X_1,\dots,X_n$에 의해서 정의된다는 점이다. 이를 강조하기 위해서 ${\boldsymbol X}$를 따로 표시하였다. (김우철 교수님 교재 p.278에서는 일반적인 likelihood $l(\theta)$와 구분하기 위하여 $l_n(\theta)$라는 기호를 썼다.) 아무튼 이 함수를 정의하면 $\theta$의 MLE는 아래와 같이 정의할 수 있다.
\begin{align}
\mbox{MLE of }\theta = \hat{\theta}^{MLE}=\underset{\theta \in \Theta}{\operatorname{ argmax} }~l(\theta;{\boldsymbol X}).
\end{align}
그런데 아래들 자체를 하나의 랜덤변수로 볼 수 있다. 
\begin{align}
\log f(X_1;\theta), \dots, \log f(X_n;\theta). 
\end{align}
이런논리면 $l(\theta;\{\boldsymbol X})$는 $n$개의 서로독립이고 동일한 분포를 따르는 확률변수의 합으로 볼 수 있다. 따라서 LLN을 쓰면 아래가 성립한다. 
\begin{align}
\frac{1}{n} \sum_{i=1}^{n} \log f(X_i;\theta)\quad \overset{p_{\theta_0} }{\operatorname \rightarrow} \quad E_{\theta_0} \log f(X_1;\theta).
\end{align}
따라서 다음과 같은 추측을 할 수 있다. 
\begin{align}
\underset{\theta \in \Theta}{\operatorname{ argmax} }\frac{1}{n}\sum_{i=1}^{n} \log f(X_i; \theta)\quad \overset{p_{\theta_0}}{\operatorname \rightarrow} \quad \underset{\theta \in \Theta}{\operatorname{argmax} } E_{\theta_0} \log f(X_1;\theta)~?
\end{align}

- 이 추측에서 좌변은 아래와 같이 정리된다. 
\begin{align}
\underset{\theta \in \Theta}{\operatorname{ argmax} }\frac{1}{n}\sum_{i=1}^{n} \log f(X_i; \theta)=\underset{\theta \in \Theta}{\operatorname{ argmax} }\sum_{i=1}^{n} \log f(X_i; \theta)=\hat{\theta}^{MLE}
\end{align}

- 이 추측에서 우변은 아래와 같이 정리된다. 
\begin{align}
\underset{\theta \in \Theta}{\operatorname{argmax} }E_{\theta_0} \log f(X_1;\theta)= \theta_0
\end{align}
이게 성립하는 이유는 쿨백라이블러 괴리도의 성질 중 아래를 이용한 것이다. (참고로 이 성질을 항상 성립하는 것은 아니며 고려하는 확률분포족이 $R0$, $R1$을 만족해야한다. 이 조건은 조금 아래에서 다룬다.)
\begin{align}
KL(\theta,\theta_0 )=0 \quad \Leftrightarrow \quad \theta=\theta_0
\end{align}

- 따라서 우리의 추측은 ((R0),(R1) 가정하에) 아래로 요약된다. 
\begin{align}
\hat{\theta}^{MLE} \overset{p_{\theta_0} }{\rightarrow} \theta_0
\end{align}
이 추측은 최대가능도추정량이 일치성을 가짐을 의미한다. 

- 잠시 메트릭의 성질에 대하여 복습하자. 메트릭은 네가지 성질은 (1) ***non-negativity*** (2) ***identity of indiscernibles*** (3) ***symmetry***, (4) ***subadditivity*** 이다. 이중에서 (1),(2) 는 묶어서 ***positive definite function*** 이라고 부르기도 한다. 그리고 ***subadditivity***를 ***triangle inequality*** 라고 부르기도 한다. 

- 아무튼 쿨백라이블러-다이버전스는 메트릭의 성질중에서 (1),(2) 즉 ***non-negativity***, ***identity of indiscernibles*** 두가지만 만족한다. 그런데 이 두가지도 항상 만족하는 것이 아니고 확률분포들의 모임
\begin{align}
\mbox{ family of pdf }:=\big\\{f(x;\theta): \theta \in \Theta \big\\}
\end{align}
이 아래의 조건을 만족해야만 한다. <br/>
**(R0) family of p.d.f. is identifiable.** <br/>
**(R1) family of p.d.f. has common support, i.e., $\mbox{supp}(x):=\\{x:f(x;\theta)>0\\}$ is not depend on  $\theta\in \Theta$.**

- 이제 다시 원래 작업으로 돌아오자. 우리가 원하는 것은 (R0),(R1) 가정하에 아래와 같이 됨을 증명하는 것이다. 
\begin{align}
\hat{\theta}^{MLE} \overset{p_{\theta_0} }{\rightarrow} \theta_0
\end{align}
핵심은 극한과 argmax함수의 교환인데 일반적으로 이를 다루는것이 MCE이다. MCE의 조건을 살펴보면 아래와 같다. (이때 $D_0(\theta):=E_{\theta_0} \log f(X_1;\theta)$, $D_n(\theta):=\frac{1}{n}\sum_{i=1}^{n}\log f(X_i;\theta)$와 같이 설정한다.) <br/><br/>
**(1)** $\theta_0$는 특정한 식 $D_0(\theta)$를 최소화 하여 구할 수 있음을 알고 있다. <br/>
**(2)** $\theta_0$는 $D_0(\theta)$를 유일하게 최소화한다. 혹은 유일하게 최소화하는 *compact set* $\cal C$를 잡을 수 있다. <br/>
**(3)** $D_0(\theta)$는 *compact set* ${\cal C}$에서 연속이다. <br/>
**(4)** 아래식을 만족하는 $D_n(\theta)$를 우리가 알고 있다. 
\begin{align}
\underset{\theta \in {\cal C}}{\operatorname{sup}}\left|D_n(\theta) - D_0(\theta)\right|  \overset{p_{\theta_0} }{\rightarrow} 0  ~~ as ~ n\rightarrow \infty
\end{align}
우리는 위의 조건에서 (1)이 성립함을 보이기 위해 (R0), (R1)를 사용하였다. (2)와 (3)에서 사용되는 compact 조건을 해결하기 위해서 **(R2) parameter space $\Theta$ is open in $\mathbb{R}^k$** 를 가정하자. 이를 가정하면 임의의 $\theta$를 포함하는 openball이 항상 존재하게 되어서 (2)와 (3)의 compact조건을 해결할 수 있다. 

- 그 다음에는  **(R3) for all realization of $(X_1,\dots,X_n)$, the second derivative of log-likelihood function exist and is continuous, i.e., $\frac{\partial^2}{\partial^2 \theta}\frac{1}{n}\sum_{i=1}^{n}l(\theta;x_i)$ (exist and) is continuous** 임을 가정하자. 참고로
\begin{align}
l(\theta,x_i) \neq l(\theta,X_i)
\end{align}
이므로 위의 조건은 $\frac{\partial^2}{\partial^2 \theta}D_n(\theta)$이 연속이라는 조건과 다름을 명심하자. 아무튼 조건 (R3)은 (3)을 imply 한다. 왜냐하면 모든 realization에 대하여 연속이면 그 realization들의 평균에 대하여서도 연속일것이기 때문이다.  

- 남은것은 (2)와 (4)이다. 그런데 ***expectation of $l(\theta;X_1)$***이 $\theta$에 대하여 strictly convex이고 (R3)이 성립하면 (4)가 성립한다는 것이 Pollard 1991에 의해서 증명되었다.  또한 ***expectation of $l(\theta;X_1)$***가 strictly convex임을 증명하면 (2)는 저절로 증명되는 셈이다. 따라서 이제 목표는 명확해졌다. 

- 우리의 목표는 아래를 증명하는 것이다. 
\begin{align}
-\frac{\partial^2}{\partial^2 \theta}E_{\theta_0} l(\theta;X_1)\underset{p.d.}{>}0
\end{align}
그런데 (R0)~(R3) 에서 추가적으로 아래를 가정하면 <br/><br/>
**(R4) integral and derivative interchangeable** <br/>
**(R5) for all $\theta \in \Theta$, there exist Fisher-information $I(\theta)$ and $I^{-1}(\theta)$**<br/><br/>
***score function(=derivative of log-likelihood)***의 성질에 의해서 아래가 성립함을 알고 있다. 
\begin{align}
\begin{cases}
E_{\theta_0}\big(\frac{\partial}{\partial \theta}\log f(X_1;\theta) \big)=0 \\\\ \\
I(\theta)=V_{\theta_0}\big(\frac{\partial}{\partial \theta}\log f(X_1;\theta)\big)
=E_{\theta_0}\big(-\frac{\partial^2}{\partial^2 \theta} \log f(X_1;\theta)\big)
\end{cases}
\end{align}
그런데 우리가 관심있는 $-\frac{\partial^2}{\partial^2 \theta}E_{\theta_0} l(\theta;X_1)$는 (R4) 가정하에 $I(\theta)$와 같고 $I(\theta)$는 (R0)~(R5) 하에서 non-negative definite matrix 이다. 따라서 $I(\theta)$를 positive definite 로 만들어 주는 추가조건 (R6) 을 사용하면 증명이 끝난다. 김우철 교수님의 교재에서는 아래와 같이 (R6)을 설정하였다. <br/><br/>
**(R6) $\hat{\theta}^{MLE}$ is unique solution of $\frac{1}{n}\sum_{i=1}^{n}\frac{\partial}{\partial\theta}\log f(X_i;\theta)=0$.**<br/>

- 결국 (R0)~(R6)를 가정하면 최대우도추정량이 일치추정량임을 MCE를 사용해서 보일 수 있다. 