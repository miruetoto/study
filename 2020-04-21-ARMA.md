---
layout : post 
title : (정리) ARMA 
---

### About this doc

- 시계열 강의노트를 각색하여 만듬. 

- 정상성의 개념을 재정립함. 

- causality에 대한 내용이 있음. 

- 학부수준을 넘어서는 내용. 


### Wold의 관찰 (교재 5.2.4.)
- Wold는 임의의 정상확률과정이 결정적성분과 비결정적 성분의 합으로 구분가능하다고 주장하였다. 즉 임의의 정상확률과정 $X_t$는 
\begin{align}
\begin{cases}
X_t = P_t + Z_t\\\\ \\
Z_t = \epsilon_t+\psi_{1}\epsilon_{t-1}+\psi_{2}\epsilon_{t-2}+\dots =\sum_{j=0}^{\infty}\psi_j \epsilon_{t-j}
\end{cases}
\end{align}
와 같이 표현할 수 있다. 

- 여기에서 $P_t$ predictable한 성분을 의한다. 구체적으로는 $x_{t-1},x_{t-2},\dots$이 주어졌을 경우 $P_t$의 값을 완벽히 예측할 수 있을때 $P_t$를 deterministic하다고 한다. $P_t$가 랜덤이 아니라는 의미가 아니라는 것에 유의하자. 

- 여기에서 $\psi_0,\dots,\psi_{\infty}$의 값은 유일하게 결정할 수 있다. 이때 $\psi_0=1$이다. 참고로 이 정리의 훌륭한 부분은 **유일하게** 결정할 수 있다는 부분이다. 

- 우리의 수준에서는 임의의 정상시계열 $\\{Z_t\\}$에 대하여 아래의 두 식을 만족하는 계수 $\psi_j$를 선택할 수 있다고 이해할 수 있다. 
\begin{align}
\begin{cases}
Z_t= \epsilon_t+\psi_{1}\epsilon_{t-1}+\psi_{2}\epsilon_{t-2}+\dots =\sum_{j=0}^{\infty}\psi_j \epsilon_{t-j}\\\\ \\
\sum_{j=0}^{\infty}\|\psi_j\|<\infty
\end{cases}
\end{align}

- 조건 
\begin{align}
\sum_{j=0}^{\infty}\|\psi_j\|<\infty
\end{align}
이 성립해야하는 이유는 $Z_t$의 분산이 유한해야 하기 때문이다. 즉 
\begin{align}
V(Z_t)=\sum_{j=0}^{\infty}\psi_j^2<\infty
\end{align}
이어야 하는데 이 조건은 $\sum_{j=0}^{\infty}\|\psi_j\|<\infty$이 만족한다면 성립하게 된다. 

- 이 정리를 쉽게 생각하면 매우 쉽게 이해할 수 있다. $Z_t=0.5Z_{t-1}+\epsilon_t$에 대하여 
\begin{align}
Z_t=\epsilon_t+0.5\epsilon_{t-1}+0.5^2\epsilon_{t-2}+\dots 
\end{align}
꼴로 정리되는 것은 이미 여러번 보였는데 이것이 그 예라고 할 수 있다. 

### MA 

- 이때  
\begin{align}
Z_t =\epsilon_t+\psi_{1}\epsilon_{t-1}+\psi_{2}\epsilon_{t-2}+\dots =\sum_{j=0}^{\infty}\psi_j \epsilon_{t-j}
\end{align}
의 꼴은 무한MA과정, 즉 MA($\infty$)로 부르기도 한다. 

- 참고로 MA(1)과정은 
\begin{align}
Z_t =\epsilon_t-\theta_{1}\epsilon_{t-1}
\end{align}
으로 표현하며 MA(2)과정은 
\begin{align}
Z_t =\epsilon_t-\theta_{1}\epsilon_{t-1}-\theta_{2}\epsilon_{t-2}
\end{align}
와 같이 표현한다. 

### equals in mean square sense 

- AR모델의 MA식 표현하는 과정에 대하여 좀 더 자세히 알아보자. 

- AR(1) 모델: 
\begin{align}
Z_t=&\phi Z_{t-1}+\epsilon_t= \phi(\phi Z_{t-2}+\epsilon_{t-1})+\epsilon_t\\\\ \\
=&\phi^L Z_{t-L}+\sum_{h=0}^{L-1}\phi^h\epsilon_{t-h}
\end{align}
결국 이런식으로 반복하면 
\begin{align}
Z_t=\sum_{h=0}^{\infty}\phi^h\epsilon_{t-h}
\end{align}
와 같이 쓸 수 있을 것 같은데 엄밀하게 말하면 $\lim_{L\to\infty}\phi^{L}Z_{t-L}=0$ 이 성립해야 한다. 

- 만약에 $Z_{t-L}$가 (아무리 커도 어떠한 고정된) 상수라면 
$\|\phi\|<1$이라는 제약아래에서 
\begin{align}
\lim_{L\to\infty}\phi^{L}Z_{t-L}=0
\end{align}
이 성립하겠다. 고정된 상수가 아니고 $\|Z_{t-L}\| \leq M$임을 알아도 극한이 0임을 주장할 수 있다. 하지만 $Z_{t-L}$이 확률변수이므로 이러한식의 주장을 할 수 없다. 왜냐하면 $\phi_L$가 점점 작아질때마다 우연히 $Z_{t-L}$가 점점 $\phi_L$ 보다 큰 값이 뽑힐 수도 있기 때문이다. (확률변수니까!) 

- 결국 사실 그럴 확률은 거의 없지 않겠냐? 즉 $Z_{t-L}$가 큰 값이 연속적으로 계속 뽑힐 **확률**이 너무 적지 않겠냐? 라는 식의 논리전개를 해야한다. 따라서 **확률, 평균**적 개념을 넣어야 한다. 결론적으로 말하면 $L\to \infty$일때 아래와 같이 평균적인 개념을 통하여서만 $Z_t=\sum_ {h=0}^{\infty} \phi^{h}\epsilon_ {t-h}$ 임을 주장할 수 있다. 
\begin{align}
\lim_{L \to \infty} E\left(Z_t-\sum_{h=0}^{L}\phi^h \epsilon_{t-h}\right)^2=0 
\end{align}
참고로 위의 식은 성립한다. 따라서 
\begin{align}
Z_t \overset{L_2}{=} \sum_{h=0}^{\infty}\phi^{h}\epsilon_{t-h}
\end{align}
와 같이 표현할 수 있다. 이때 등호는 **mean square sense**에서 성립한다고 표현한다. 그런데 일반적으로 간략하게는 
\begin{align}
Z_t = \sum_{h=0}^{\infty}\phi^{h}\epsilon_{t-h}
\end{align}
와 같이 표현한다. 

- 잘 생각해보면 저 등호가 진짜 우리가 $1+1=2$ 같은걸 표현할때의 등호가 아니라는 것은 쉽게 알 수 있다. $X$, $Y$ 모두 확률변수이면 애초에 
\begin{align}
X=Y
\end{align}
와 같은 표현이 말이 안된다. 그래서 
\begin{align}
X\overset{d}{=}Y
\end{align}
와 같이 쓰든가 해야한다. 

### causality 

- AR(1) 모델인데 $\|\phi\|>1$인 경우는 어떻게 될까? 당연히 
\begin{align}
\lim_{L\to\infty}\phi^{L}Z_{t-L}
\end{align}
이 수렴하지 않기 때문에 
\begin{align}
Z_t = \sum_{h=0}^{\infty}\phi^{h}\epsilon_{t-h}
\end{align}
와 같이 표현할 수 없겠다. 그런데 만약에 아래와 같이 $Z_t$를 표현해보면 어떨까?
\begin{align}
Z_{t+1}=\phi Z_t +\epsilon_t\\\\ \\
Z_t=\frac{1}{\phi}Z_{t+1}+\frac{1}{\phi}\epsilon_t
\end{align}
그렇다면 
\begin{align}
Z_t=\frac{1}{\phi^L}Z_{t+L}-\sum_{h=1}^{L-1}\frac{1}{\phi^h}\epsilon_{t+h}
\end{align}
와 같이 표현된다. 일반적인 AR모델은 과거에서부터 순차적으로 미래로 확률변수들을 생성하는데, 이 모델은 미래에서 순차적으로 확률변수들을 생성한다고 생각하면 된다. 이런방식으로 무한한 과거상태로 간다면 이 모델은 정상상태에 있다고 주장할 수 있다. 

- 따라서 어떤사람들은 이 모델을 정상모델이라고 주장하기도 한다. 이러한 모델을 ***stationary future dependent AR(1) model*** 이라고 한다. 이 논리대로 치면 모든 AR(1) 모델은 $\phi=1$인 경우를 제외하고 정상모델이다. 그런데 사실상 $\|\phi\|>1$인 경우의 모델은 쓸모가 없는데 미래의 값으로 과거의 값을 예측하는게 의미가 없기 때문이다. 이렇게 미래에서 과거의 값을 예측하는 모델을 ***causal***하지 않다고 말한다. 따라서 *stationary future dependent AR(1) model*을 주장하는 사람들은 정상성외에 ***causality***를 추가적으로 가정한다. 결국 이 부류의 사람들도 관심이 있는 모델은 $\|\phi\|<1$인 경우이다. 

- 참고로 *stationary future dependent AR(1) model*을 인정하는 사람들은 울드의 정리도 아래와 같은 형태로 이해한다. 
\begin{align}
\begin{cases}
Z_t=\sum_{j=\-\infty}^{\infty}\psi_j \epsilon_{t-j}\\\\ \\
\sum_{j=-\infty}^{\infty}\|\psi_j\|<\infty
\end{cases}
\end{align}

- 위의 표현방법을 수용하면 AR(1)에 한정하여 $\|\phi\|>1$인 모델은 그에 대응하는 (그와 분포가 같은) *causal* 모델로 바꿀 수 있다. 구체적으로 
\begin{align}
Z_t=2 Z_{t-1}+\epsilon_t, \quad \epsilon_t \sim ~iid~N(0,1)
\end{align}
를 만족하는 시계열 $\\{Z_t\\}$ 는 
\begin{align}
Y_t=0.5 Y_{t-1}+\eta_t, \quad \eta_t \sim ~iid~ N(0,1/4)
\end{align}
를 만족하는 시계열 $\\{Y_t\\}$와 **동등**하다. 아래를 관찰하라.
\begin{align}
Z_t \overset{L_2}{=}(\epsilon_t/2)+0.5(\epsilon_{t+1}/2)+0.5^2(\epsilon_{t+2}/2)+\dots\\\\ \\
Y_t \overset{L_2}{=}\eta_t+0.5\eta_{t-1}+0.5^2\eta_{t-2}+\dots
\end{align}
즉 모든 유한개의 샘플에 대하여 분포가 같다. 이러한 개념은 쉽게 고차원으로 확장할 수 있다. 

- *stationary future dependent AR(1) model*의 개념을 받아들이면 편리한점이 있는데 바로 위와 같은 예제이다. 만약에 *stationary future dependent AR(1) model*을 비정상시계열로 받아들인다면 위의 예제에서 $\\{Z_t\\}$는 비정상시계열이고 $\\{Y_t\\}$는 정상시계열인데 두 시계열은 분포가 같다는 이상한 결론이 나온다. *stationary future dependent AR(1) model*을 받아들이면 이러한 개념적인 모순을 피할 수 있다. 대신에 현재의 값을 미래의 관측치들로 표현해야 한다는 모순이 있다. 

- 사실 잘 생각해보면 $\|\phi\|>1$인 경우에도 분산구조는 lag에만 의존한다. (평균은 $0$ 이라 가정한다) 이는 정상시계열의 조건에 딱 맞다. 따라서
\begin{align}
Z_t=2Z_{t-1}+\epsilon_t
\end{align}
와 같은 모델은 정상시계열이다. 그럼에도 불구하고 $\phi$의 절대값이 1보다 큰 시계열은 주구장창 비정상시계열이라고 주장했던 이유는 $t\to \infty$ 일수록 분산이 터지기 때문이다. 

- 하지만 $t \to -\infty$ 이라면 분산이 터지지 않는다. 즉 **분산이 터진다** 라는 표현자체가 교묘한데 마치 무한개 더해서 분산이 터지는것처럼 착각을 하게 만들지만 실제로는 더하는 횟수가 무한임이 중요한게 아니고 더하는 방향이 중요한 것이었다. 만약에 미래에서부터 과거로 살아가는 사람이 $\|\phi\|>1$ 인 AR(1)모형을 관찰한다면 즉 
\begin{align}
Z_t=2Z_{t-1}+\epsilon_t
\end{align}
와 같은 모형을 관찰한다면 이는 점점 안정화되는 정상시계열의 모습을 가진다고 느낄 것이다. 

- **아무튼 미래에서부터 오는 정상같은 개념은 분석하지 않는다.**

### invertable MA

- 이제 MA모델로 넘어가 보자. 우선 $\\{Z_t\\}$가 아래와 같이 표현되는 모델을 MA(q)라고 한다. 
\begin{align}
Z_t=\epsilon_t + \theta_1 \epsilon_{t-1}+\dots+\theta_q\epsilon_{t-q}
\end{align}
교재에 따라서 아래와 같이 정의하기도 한다. 
\begin{align}
Z_t=\epsilon_t - \theta_1 \epsilon_{t-1}-\dots-\theta_q\epsilon_{t-q}
\end{align}
울드의 정리에 따르면 이것은 항상 정상과정시계열이다. (유한차수의 MA모델은 항상 정상이다)

- AR모델과 MA모델은 사실상 표현의 차이라고 이해해도 된다. $\phi=0.5$인 AR(1) 모형은 무한차수의 MA모형으로 표현할 수 있다. 사실상 좌변에 $Z_t$를 하나만 두고 우변에 $\\{\dots,\epsilon_{t-1},\epsilon_t,\epsilon_{t+1},\dots \\}$ 따위를 몰아넣고 정리하면 MA를 모델이 되고 우변에 하나의 $\epsilon_t$를 두고 좌변에 $\\{\dots,Z_{t-1},Z_t,Z_{t+1},\dots\\}$ 따위를 몰아넣으면 AR모델처럼 느껴지는데 실제로 그렇다. 편의상 아래와 같이 생각할 수 있다. 
\begin{align}
\begin{cases}
\mbox{AR식 표현: } \mbox{function of } \Big\\{ \dots,Z_{t-1},Z_t,Z_{t+1},\dots \Big\\} = \epsilon_t \\\\ \\
\mbox{MA식 표현: } Z_t = \mbox{function of } \Big\\{ \epsilon_{t-1},\epsilon_t,\epsilon_{t+1},\dots \Big\\}
\end{cases}
\end{align}

- 같은 ACF를 가지는 MA모델이 존재함이 알려져 있다. 구체적으로 아래의 두 시계열을 고려하자. 
\begin{align}
Z_t=\epsilon_t+0.2\epsilon_{t-1}\\\\ \\
Y_t=\eta_t+5\eta_{t-1}\\\\ \\
\end{align}
이때 $\epsilon_t \sim N(0,25)$ 이고 $\eta_t \sim N(0,1)$이라고 가정하자. 두 모델 모두 평균은 0이고 acf로 아래를 가진다. 
\begin{align}
\gamma(h)=\begin{cases}
26 & h=0\\\\ \\
5 & h=1\\\\ \\
0 & h>1
\end{cases}
\end{align}
평균은 0일테고 acf가 lag에만 의존하고 분산은 유한개를 더한값이 당연히 터지지 않는다. 따라서 두 시계열은 모두 정상이다. 심지어 정규분포가정을 하였으므로 두 시계열은 임의의 유한개 샘플을 뽑았을 경우 분포가 같다. 즉 동등하다. 이미 AR에서 더 악랄한 예제를 경험했기 때문에 위의 결과가 그다지 충격적이진 않다. 

- 당연히 해결책이 있다. 해결책 역시 AR의 경우와 유사한데 하나의 모델을 버린다. 이 예제에서는 $\\{Y_t\\}$를 버린다. $\\{Y_t\\}$를 버리는 이유를 알아보기 위해서 $\\{Z_t\\}$와 $\\{Y_t\\}$ 모두 AR식으로 표현해보자. (위의 모델에서 $Z_t\leftrightarrow \epsilon_t$를 잠시 착각하여 본 뒤 서로 되돌리면 쉽게 아래 표현을 얻음)
\begin{align}
\epsilon_t=Z_t+(-0.2)Z_{t-1}+(-0.2)^2 Z_{t-2}+\dots \\\\ \\ 
\eta_t=Y_t+(-5)Y_{t-1}+(-5)^2 Y_{t-2}+\dots 
\end{align}
딱봐도 $Y_t$를 버려야 할 것처럼 생겼다. 

- 위에서 등호는 사실 진짜 등호가 아니라 $\overset{L_2}{=}$임을 기억하자. 따라서 등호를 성립하게 하려면 오히려 
\begin{align}
\eta_t=Y_t+(-0.2)Y_{t+1}+(-0.2)^2 Y_{t+2}+\dots 
\end{align}
와 같은 식으로 표현해야 할 것 같은데 이러한 표현은 causal 하지 않다. 따라서 $\\{Y_t\\}$를 버린다. 이때 버려진 $\\{Y_t\\}$는 invertable 하지 않은 MA라고 하고 $\\{Z_t\\}$는 invertable 한 MA라고 표현한다. 

- $Z_t\leftrightarrow \epsilon_t$를 잠시 바꾸어 생각하면 $\overset{L_2}{=}$를 만족시키기 위한 논리전개를 AR과 동등하게 할 수 있다. 따라서 MA과정이 invertable하기 위한 조건은 
\begin{align}
Z_t=\boldsymbol{\theta}(B)\epsilon_t
\end{align}
꼴로 표현하였을 경우 $\boldsymbol{\theta}(B)=0$의 근의 절대값이 모두 단위원 밖에 있어야 한다는 조건과 같음을 쉽게 유추할 수 있다. 

### daulity 

- 이와 유사하게 대부분의 경우에 $Z_t\leftrightarrow \epsilon_t$를 잠시 바꾸어서 생각해본뒤에 논리전개를 하면 AR과정에서 공부하였던 여러가지 성질들을 MA과정으로 바꾸어 생각할 수 있게 된다. 이러한 성질을 쌍대성이라고 부른다. 

- 예를들면 causal and stationary AR(1)의 ACF가 
지수적으로 감소하고 PACF는 lag=1에서만 존재함을 우리는 알고 있다. 

- 그런데 이를 통하여 invertable한 MA(1)과정의 PACF는 지수적으로 감소하고 ACF는 lag=1에서만 존재함을 유추할 수 있는데 이것이 바로 쌍대성을 이용한 것이다. 

### ARMA

- AR과 MA가 섞인 모델을 ARMA모델이라고 하는데 예를들면 아래와 같은 모델이다. 
\begin{align}
Z_t-\phi_1Z_{t-1}-\dots-\phi_pZ_{t-p}=\epsilon_t-\theta_1\epsilon_{t-1}-\dots-\theta_q\epsilon_{t-q}.
\end{align}
Back shift operator를 사용하면 
\begin{align}
\boldsymbol{\phi}(B)Z_t=\boldsymbol{\theta}(B)\epsilon_t
\end{align}
와 같이 표현된다. 이런모델을 ARMA(p,q)라고 한다. 

- 이 모델은 당연히 AR식 혹은 MA식으로 표현가능할텐데 그것의 등호가 mean square sense에서 의미가 있으려면 
\begin{align}
\begin{cases}
\boldsymbol{\phi}(B)=0\\\\ \\
\boldsymbol{\theta}(B)=0
\end{cases}
\end{align}
의 근들의 절대값이 모두 1보다 커야한다. 

- 참고로 $\boldsymbol{\phi}(B)=0$의 근들의 절대값이 모두 1보다 크면 stationary and causal ARMA model 이라고 부른다. 혹은 그냥 정상인 AR모형이라고 한다.

- $\boldsymbol{\theta}(B)=0$의 근들이 절대값이 모두 1보다 크면 invertable한 ARMA 모델이라고 부른다. 

- 따라서 우리가 관심이 있는 것은 stationary and causal 하고 invertable한 ARMA 모델이 된다. 혹은 줄여서 stationary 하고 invertable 한 ARMA 모델에 관심이 있다고 할 수 있다. 

- AR(p)모형은 ARMA(p,0)모델과 같다. MA(q)모형은 ARMA(0,q)모형과 같다. 울드의 정리는 모든 정상시계열이 유한차수의 ARMA모델로 꽤 정확히 근사시킬 수 있음을 암시한다. 

- 따라서 결론적으로 ARMA모델을 잘 적합시키면 시계열분석은 끝난다. ARMA모델을 적합시키는 방법은 ACF와 PACF의 그림을 잘 분석하면 된다. ARMA모델에서 ACF와 PACF의 그림이 어떻게 나오는지는 교재 p.241, 표 6-1에 요약되어있다. 

