\documentclass[12pt,oneside,english,a4paper]{article}
\usepackage{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{color}
\usepackage{graphicx}
\usepackage{wallpaper}
\usepackage{wrapfig,booktabs}

\usepackage{fancyhdr}

\usepackage{fourier-orns}
\newcommand{\dash}{\noindent \newline\textcolor{black}{\hrulefill~ \raisebox{-2.5pt}[10pt][10pt]{\leafright \decofourleft \decothreeleft  \aldineright \decotwo \floweroneleft \decoone   \floweroneright \decotwo \aldineleft\decothreeright \decofourright \leafleft} ~  \hrulefill}}

\usepackage{titlesec}
\titleformat*{\section}{\it\huge\bfseries}
\titleformat*{\subsection}{\it\huge\bfseries}
\titleformat*{\subsubsection}{\it\LARGE\bfseries}
\titleformat*{\paragraph}{\huge\bfseries}
\titleformat*{\subparagraph}{\LARGE\bfseries}

\usepackage[left=20px,right=20px,top=50px,bottom=50px,paperwidth=8in,paperheight=12in]{geometry}

\usepackage[cjk]{kotex}
\usepackage{amsthm} 
\usepackage{amsmath} 
\usepackage{amsfonts}
\usepackage{enumerate} 
\usepackage{cite}
\usepackage{graphics} 
\usepackage{graphicx,lscape} 
\usepackage{subcaption}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{titlesec}
\usepackage{cite, url}
\usepackage{amssymb}

\def\bk{\paragraph{\Large$$}\Large}
\def\ck{\paragraph{\Large$\bullet$}\Large}
\def\sol{\paragraph{\Large(sol)}\Large}
\def\pf{\paragraph{\Large(pf)}\Large}
\def\note{\paragraph{\Large\textit{\underline{note:}}}\Large}
\def\ex{\paragraph{\Large\textit{example:}}\Large}
\newcommand{\para}[1]{\paragraph{\Large\it\underline{\textbf{#1:}}}\Large}
\newcommand{\parablue}[1]{\paragraph{\Large\textcolor{blue}{\it\underline{\textbf{#1:}}}}\Large}
\newcommand{\parared}[1]{\paragraph{\Large\textcolor{red}{\it\underline{\textbf{#1:}}}}\Large}


\def\one{\paragraph{\Large(1)}\Large}
\def\two{\paragraph{\Large(2)}\Large}
\def\three{\paragraph{\Large(3)}\Large}
\def\four{\paragraph{\Large(4)}\Large}
\def\five{\paragraph{\Large(5)}\Large}
\def\six{\paragraph{\Large(6)}\Large}
\def\seven{\paragraph{\Large(7)}\Large}
\def\eight{\paragraph{\Large(8)}\Large}
\def\nine{\paragraph{\Large(9)}\Large}
\def\ten{\paragraph{\Large(10)}\Large}

\def\cka{\paragraph{\Large(a)}\Large}
\def\ckb{\paragraph{\Large(b)}\Large}
\def\ckc{\paragraph{\Large(c)}\Large}
\def\ckd{\paragraph{\Large(d)}\Large}
\def\cke{\paragraph{\Large(e)}\Large}
\def\ckf{\paragraph{\Large(f)}\Large}
\def\ckg{\paragraph{\Large(g)}\Large}
\def\ckh{\paragraph{\Large(h)}\Large}
\def\cki{\paragraph{\Large(i)}\Large}
\def\ckj{\paragraph{\Large(j)}\Large}

\newcommand{\bs}[1]{\mbox{\boldmath $#1$}}

\newcommand{\bsa}{\mbox{\boldmath $a$}}
\newcommand{\bsb}{\mbox{\boldmath $b$}}
\newcommand{\bsc}{\mbox{\boldmath $c$}}
\newcommand{\bsd}{\mbox{\boldmath $d$}}
\newcommand{\bse}{\mbox{\boldmath $e$}}
\newcommand{\bsf}{\mbox{\boldmath $f$}}
\newcommand{\bsg}{\mbox{\boldmath $g$}}
\newcommand{\bsh}{\mbox{\boldmath $h$}}
\newcommand{\bsi}{\mbox{\boldmath $i$}}
\newcommand{\bsj}{\mbox{\boldmath $j$}}
\newcommand{\bsk}{\mbox{\boldmath $k$}}
\newcommand{\bsl}{\mbox{\boldmath $l$}}
\newcommand{\bsm}{\mbox{\boldmath $m$}}
\newcommand{\bsn}{\mbox{\boldmath $n$}}
\newcommand{\bso}{\mbox{\boldmath $o$}}
\newcommand{\bsp}{\mbox{\boldmath $p$}}
\newcommand{\bsq}{\mbox{\boldmath $q$}}
\newcommand{\bsr}{\mbox{\boldmath $r$}}
\newcommand{\bss}{\mbox{\boldmath $s$}}
\newcommand{\bst}{\mbox{\boldmath $t$}}
\newcommand{\bsu}{\mbox{\boldmath $u$}}
\newcommand{\bsv}{\mbox{\boldmath $v$}}
\newcommand{\bsw}{\mbox{\boldmath $w$}}
\newcommand{\bsx}{\mbox{\boldmath $x$}}
\newcommand{\bsy}{\mbox{\boldmath $y$}}
\newcommand{\bsz}{\mbox{\boldmath $z$}}

\newcommand{\bfa}{\mbox{$\bf{a}$}}
\newcommand{\bfb}{\mbox{$\bf{b}$}}
\newcommand{\bfc}{\mbox{$\bf{c}$}}
\newcommand{\bfd}{\mbox{$\bf{d}$}}
\newcommand{\bfe}{\mbox{$\bf{e}$}}
\newcommand{\bff}{\mbox{$\bf{f}$}}
\newcommand{\bfg}{\mbox{$\bf{g}$}}
\newcommand{\bfh}{\mbox{$\bf{h}$}}
\newcommand{\bfi}{\mbox{$\bf{i}$}}
\newcommand{\bfj}{\mbox{$\bf{j}$}}
\newcommand{\bfk}{\mbox{$\bf{k}$}}
\newcommand{\bfl}{\mbox{$\bf{l}$}}
\newcommand{\bfm}{\mbox{$\bf{m}$}}
\newcommand{\bfn}{\mbox{$\bf{n}$}}
\newcommand{\bfo}{\mbox{$\bf{o}$}}
\newcommand{\bfp}{\mbox{$\bf{p}$}}
\newcommand{\bfq}{\mbox{$\bf{q}$}}
\newcommand{\bfr}{\mbox{$\bf{r}$}}
\newcommand{\bfs}{\mbox{$\bf{s}$}}
\newcommand{\bft}{\mbox{$\bf{t}$}}
\newcommand{\bfu}{\mbox{$\bf{u}$}}
\newcommand{\bfv}{\mbox{$\bf{v}$}}
\newcommand{\bfw}{\mbox{$\bf{w}$}}
\newcommand{\bfx}{\mbox{$\bf{x}$}}
\newcommand{\bfy}{\mbox{$\bf{y}$}}
\newcommand{\bfz}{\mbox{$\bf{z}$}}

\newcommand{\bsA}{\mbox{\boldmath $A$}}
\newcommand{\bsB}{\mbox{\boldmath $B$}}
\newcommand{\bsC}{\mbox{\boldmath $C$}}
\newcommand{\bsD}{\mbox{\boldmath $D$}}
\newcommand{\bsE}{\mbox{\boldmath $E$}}
\newcommand{\bsF}{\mbox{\boldmath $F$}}
\newcommand{\bsG}{\mbox{\boldmath $G$}}
\newcommand{\bsH}{\mbox{\boldmath $H$}}
\newcommand{\bsI}{\mbox{\boldmath $I$}}
\newcommand{\bsJ}{\mbox{\boldmath $J$}}
\newcommand{\bsK}{\mbox{\boldmath $K$}}
\newcommand{\bsL}{\mbox{\boldmath $L$}}
\newcommand{\bsM}{\mbox{\boldmath $M$}}
\newcommand{\bsN}{\mbox{\boldmath $N$}}
\newcommand{\bsO}{\mbox{\boldmath $O$}}
\newcommand{\bsP}{\mbox{\boldmath $P$}}
\newcommand{\bsQ}{\mbox{\boldmath $Q$}}
\newcommand{\bsR}{\mbox{\boldmath $R$}}
\newcommand{\bsS}{\mbox{\boldmath $S$}}
\newcommand{\bsT}{\mbox{\boldmath $T$}}
\newcommand{\bsU}{\mbox{\boldmath $U$}}
\newcommand{\bsV}{\mbox{\boldmath $V$}}
\newcommand{\bsW}{\mbox{\boldmath $W$}}
\newcommand{\bsX}{\mbox{\boldmath $X$}}
\newcommand{\bsY}{\mbox{\boldmath $Y$}}
\newcommand{\bsZ}{\mbox{\boldmath $Z$}}

\newcommand{\bfA}{\mbox{$\bf{A}$}}
\newcommand{\bfB}{\mbox{$\bf{B}$}}
\newcommand{\bfC}{\mbox{$\bf{C}$}}
\newcommand{\bfD}{\mbox{$\bf{D}$}}
\newcommand{\bfE}{\mbox{$\bf{E}$}}
\newcommand{\bfF}{\mbox{$\bf{F}$}}
\newcommand{\bfG}{\mbox{$\bf{G}$}}
\newcommand{\bfH}{\mbox{$\bf{H}$}}
\newcommand{\bfI}{\mbox{$\bf{I}$}}
\newcommand{\bfJ}{\mbox{$\bf{J}$}}
\newcommand{\bfK}{\mbox{$\bf{K}$}}
\newcommand{\bfL}{\mbox{$\bf{L}$}}
\newcommand{\bfM}{\mbox{$\bf{M}$}}
\newcommand{\bfN}{\mbox{$\bf{N}$}}
\newcommand{\bfO}{\mbox{$\bf{O}$}}
\newcommand{\bfP}{\mbox{$\bf{P}$}}
\newcommand{\bfQ}{\mbox{$\bf{Q}$}}
\newcommand{\bfR}{\mbox{$\bf{R}$}}
\newcommand{\bfS}{\mbox{$\bf{S}$}}
\newcommand{\bfT}{\mbox{$\bf{T}$}}
\newcommand{\bfU}{\mbox{$\bf{U}$}}
\newcommand{\bfV}{\mbox{$\bf{V}$}}
\newcommand{\bfW}{\mbox{$\bf{W}$}}
\newcommand{\bfX}{\mbox{$\bf{X}$}}
\newcommand{\bfY}{\mbox{$\bf{Y}$}}
\newcommand{\bfZ}{\mbox{$\bf{Z}$}}

\DeclareMathOperator*{\argmin}{argmin} 
\DeclareMathOperator*{\argmax}{argmax} 

\usepackage[svgnames]{xcolor}
\usepackage{listings}

\lstset{language=R,
    basicstyle=\Large\tt,
    stringstyle=\color{DarkGreen},
    otherkeywords={0,1,2,3,4,5,6,7,8,9},
    morekeywords={TRUE,FALSE},
    deletekeywords={data,frame,length,as,character},
    %keywordstyle=\color{blue},
    commentstyle=\color{DarkGreen},
}

\CJKscale{0.9}
\begin{document}

\section{모델링이란?}
\ck 모델링이란 
\begin{align*}
y_i=f(x_i)+\epsilon_i
\end{align*}
의 꼴에서 $f$의 모양을 결정하는 과정을 의미한다. 

\ck $f$의 모양을 결정할때 데이터에 대한 확실한 사전정보가 있는 경우가 있다. 예를들어 
"$f(x)$는 $x$에 선형변환으로 만들어질 수 있다. (즉 $f(x)=\beta_0+\beta_1x$)"
라는 사실을 알고 있는 경우이다. 이는 
\begin{align*}
y_i=f(x_i)+\epsilon_i, 
\end{align*}
와 같은 모델에서 $f$가 어떠한 형태를 가질것인지 미리 알고 있다고 생각한다는 말과 같다. 이처럼 $f$가 어떤 모양인지 미리 알고 접근하는 방법을 파라메트릭 모델링 이라고 한다. 

\ck 사전정보가 없어서 $f$를 어떻게 모델링할지 감이 안 올 수도 있다. 즉 자료를 봤는데 선형의 모양을 가지는지 어떤지 감을 못잡겠는 경우이다. 이것을 바꾸어 말하면 $\{y_i\}$가 $\{x_i\}$의 어떤 space 에 있는지 감을 못 잡겠다는 뜻이다. 혹은 모델링이 귀찮을 수도 있다. 이럴 경우 $f(x)$가 $x$의 어떤 특정스페이스 $\cal A$의 부분공간에 존재한다고 가정하고 그 특정스페이스 $\cal A$를 할 수 있는 베이시스를 선택하여 문제를 풀 수 있다. 가령 예를들면
''$f(x)$가 어떤 공간에 있는지 모르겠는데 최소한 비숍스페이스의 부분공간에 있는것 같아'' 
라고 생각한다면 웨이블릿 베이시스를 선택하여 모델링 하는 것이다. 보통 위와 같은 접근법은 무한대의 basis 를 활용한다. 많은 수학자들이 
"이런식으로 무한개의 basis를 활용하면 특정공간에 있는 어떠한 함수도 표현할 수 있어요~"
라는 식의 증명을 많이 해놓았는데 이러한 증명결과들을 적극적으로 활용하는 셈이다. 요렇게 $f$를 표현하는데 무한개의 basis를 활용하는 모델링을 semi-parametric modeling 이라고 한다. 

\ck 웨이블릿과 퓨리에변환등으로 $f(x)$를 추론하는 것이 대표적인 세미파라메트릭 모델링이다. 

\ck 파라메트릭 모델링도 못하겠고 세미파라메트릭 모델링도 못하겠다면 넌파라메트릭 모델링을 할 수 있다. 넌파라메트릭 모델링은 $f(x)$ 에 대한 어떠한 가정도 필요하지 않다. 예를들면 "어떠한 식으로 표현가능하다" 라든가 (요건 파라메트릭 스타일) 혹은 "최소한 어떠한 공간안에 있는것 같다" 라든가 (요건 세미파라메트릭 스타일) 하는 식의 가정이 필요하지 않다. 

\ck 넌파라메트릭(non-parametric) 은 통계학에서 대충 2개의 의미로 쓰인다. 보통 (1) 자료가 특정한 분포에서 나왔다는 가정이 필요없는 경우 (2) 자료가 특정한 모델 혹은 스트럭처에서 생성된다는 가정이 없는 경우를 의미한다(위키피디아 참고). (1) 과 관련된 용어로는 non-prametric statistics, distribution-free, rank, order-statistics 등이 있다. (2) 와 관련된 용어는 non-prametric regression, non-parametric hierarchical Bayesian models, kernel 등이 있다. 여기에서는 모델링과 관련된 내용을 다루므로 (2)의 경우로 한정하자. 

\ck 넌파라메트릭은 베이시스를 설계할때 입력표본 $x_i$을 사용한다. 따라서 given data 에 따라 basis 가 달라진다. 이러한 특징을 data adaptive 하다 라고 표현한다. 아래는 넌파라메트릭 모델링의 예이다. 
\begin{align*}
f(x)=e^{-\frac{\| x-x_1\|}{2h^2}}\beta_1+e^{-\frac{\| x-x_2\|}{2h^2}} \beta_2+\dots+e^{-\frac{\| x-x_n\|}{2h^2} }\beta_n
\end{align*}
위의 식을 보면 입력데이터를 그대로 쓰지 않고 커널(kernel) 을 한번 씌웠는데 이렇게 입력표본에 커널을 씌우는 방식을 커널모델이라고 한다. 

\ck 넌파라메트릭 방법중에 커널을 쓰는 방법이 매우 많다. 하지만 모든 넌파라메트릭 방법이 반드시 커널을 써야하는 것은 아니다. 예를 들어 simple kriging 같은 경우는 커널을 쓰지 않는 넌파라메트릭 방법으로 볼 수 있다. 

\ck 참고로 넌파라메트릭 방법에도 모수가 있다. 구체적으로 위의식에서 $\beta_j$ 가 모수에 해당한다. 넌파라메트릭 방법은 모수가 있냐 없냐가 중요한게 아니라 basis 가 given data 의 함수이냐 아니냐가 중요하다. 따라서 내생각에는 넌파라메트릭 모델 보다 data-adaptive model 이 더 좋은 표현 인것 같다. 즉 다시말하면 파라메트릭 모델 혹은 세미파라메트릭 모델은 $f$의 스트럭처(structure) 가 모수 $\theta$ 만의 함수이다. 따라서 $y_i=f(x_i)+\epsilon_i)$ 의 꼴에서 $f$ 를 
\begin{align*}
f_{\theta}
\end{align*}
와 같은 형태로 표현가능하다. 하지만 넌파라메트릭 모델은 $f$의 스트럭처가 모수와 $x_i$에 모두 의존한다. 

\ck 실제로 아까 살펴본 커널모델을 다시 살펴보자. 커널을 결정하는 것은 사실 (1) 밴드윗 (2) 중심점 (3) 높이 인데 여기에서 
\begin{align*}
f(x)=e^{-\frac{\| x-x_1\|}{2h^2}}\beta_1+e^{-\frac{\| x-x_2\|}{2h^2}} \beta_2+\dots+e^{-\frac{\| x-x_n\|}{2h^2} }\beta_n
\end{align*}
아까의 모델이 넌파라메트릭인 이유는 (2) 중심점을 표현하는 파라메터가 데이터 자체이기 때문이다. 

\section{Linear model, Nonlinear model}

\ck 모델링이란 
\begin{align*}
y_i=f(x_i)+\epsilon_i
\end{align*}
의 꼴에서 $f$의 모양을 결정하는 과정을 의미한다. 

\ck 이때 $f$는 베이시스와 파라메터로 구성된다. 그런데 이때 $f$가 아래와 같이 
\begin{align*}
f = \sum basis \times coef
\end{align*}
꼴의 형태로 표현할 수 있다면 이 모델을 선형모델이라고 하고 그렇지 않으면 비선형모델이라고 한다. 

\ck 일반적으로 커널은 bandwidth 와 height 에 따라서 결정된다. 예를들어 
\begin{align*}
f(x)=e^{-\frac{\| x-x_1\|}{2h^2}}\beta_1+e^{-\frac{\| x-x_2\|}{2h^2}} \beta_2+\dots+e^{-\frac{\| x-x_n\|}{2h^2} }\beta_n
\end{align*}
와 같은 모델에서는 밴드윗이 $h$ 이고 높이가 $\beta_i$ 라고 이해할 수 있다. 만약에 $h$ 가 known 이라 $\beta_i$ 만을 추정해도 된다고 하자. 이 경우는 베이시스와 코이피션트가 선형으로 결합된 형태가 되므로 이 경우에는 커널모델을 선형모델로 해석할 수 있다. 하지만 $h$ 역시 우리가 추정해야 한다면 이 모델은 비선형모델이 된다. 

\ck 선형모델이 아니지만 적당한 변환을 통하여 선형모델로 바꿀수 있는 모델 역시 넓은범위에서는 선형모델로 친다. 이런 모델을 generalized linear model 이라고 한다. 로지스틱이나 로그선형모델등이 이러한 범주에 속한다. 

\ck 많은 사람들이 $f(x)$가 $x$에 대하여 선형이면 선형모델이라고 생각하는데 이건 사실이 아니다. 이것은 $f$가 $x$에 대하여 비선형이라도 파라메터 $\beta$에 대하여 선형이라면 선형모델이라고 한다. 중학교 1학년때 배우는 방정식을 보면 기준을 $x$로 보느냐 $y$로 보느냐에 따라 1차식이 되기도 하고 2차식이 되기도 하는 수식을 보았을 것이다. 예를 들어보자. 
\begin{align*}
x^2 y= 0
\end{align*} 
은 이 1차식인가 2차식인가? 이것은 $x$에 대한 2차식이지만 $y$에 대한 1차식이다. 이와 유사하게 
\begin{align*}
f(x)=\beta_0+\beta_1 x + \beta_2 x^2
\end{align*}
은 $x$에 대하여는 비선형이지만 $\beta=(\beta_0,\beta_1,\beta_2)'$에 대해서는 선형이다. 

\section{Additive model, Multiplicative model, Hierarchical model}

\ck 모델링이란 
\begin{align*}
y_i=f(x_i)+\epsilon_i
\end{align*}
의 꼴에서 $f$의 모양을 결정하는 과정을 의미한다. 

\ck 이때 $f$는 컴포넌트(component) 들의 조합으로 구성된다고 볼 수 있다. 여기에서 콤포넌트란 베이시스와 코이피션트가 결합된 형태로 이해하면 된다. 예를들어 
\begin{align*}
y_i=f(x_i)+\epsilon_i=\beta_0+ \beta_1x_i+\epsilon_i
\end{align*}
는 $\beta_0$ 와 $\beta_1 x_i$ 2개의 컴퍼넌트가 있다고 볼 수 있다. 

\ck 각 컴퍼넌트들이 더하기로 연결되어 있으면 additive model 이라고 한다. 각 컴퍼넌트들이 곱의 형태로 표현되어 있으면 multiplicative model 이라고 한다. 각 컴퍼넌트간에 하이라키가 있다면 Hierarchical model 이라고 한다. 3개의 분류는 서로 배타적이지는 않다. 예를들면 아래와 같은 모델을 살펴보자. 
\begin{align*}
{\bf y}=\beta_0+ X_1 \beta_1 + X_2 \beta_2 + X_1X_2 \gamma + \boldsymbol{\epsilon}
\end{align*}
이 모델은 어디티브 모델이지만 (각 컴퍼넌트들이 최종적으로는 +로 연결됨) 살짝 멀티플리케이티브 모델 느낌도 있고 ($\gamma$ 에 대응하는 베이시스는 곱의 형태로 되어있음) 하이라키 모델 ($X_1X_2$ 는 2차식이지만 $X_1$, $X_2$는 1차식이고 1차식을 다 알면 2차식을 construct 할 수 있는 구조임) 이다. 이 모델은 딱 잘라서 무슨 모델이라고 말하기 애매하다. 또한 시계열모형중 하나인 garch 와 같은 경우는 어디티브 모델과 멀티플리케이트브 모델 어느쪽으로도 해석하기 애매하다. 

\section{결론}

\ck 모델들에 대한 용어들은 서로간의 관계가 애매해서 표나 벤다이어그램으로 이해할 생각은 안하는게 좋다. 

\ck 가령 예를들면 퓨리에변환, 웨이블릿 변환은 세미파라메트릭 모델이면서 선형모델이다. 그리고 어디티브 모델이다. 또한 scale 에 따라서 계층구조가 존재하므로 하이라키 모델이다. 그리고 LDA(Latent Dirichlet allocation) 와 같은 모델은 하이라키 모델이며 넌파라메트릭 모델이다. 로지스틱은 비선형 모델이지만 generalized linear model 이다. 신경망은 파라메트릭 모델이면서 비선형 모델이고 계층모델이다. 

\end{document}

