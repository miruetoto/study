---
layout: post
title: (정리) EM 알고리즘
--- 

### About this doc 

- 통계공부

- 학부 혹은 대학원 1-2학년 수준 

- 여기에서는 EM 알고리즘에 대하여 다룬다. 이 포스팅은 임요한 교수님의 강의노트를 위주로 요약하였다. 그리고 랭이 쓴 아래의 교재로 보충하였다. <br/> 
Lange, K. (2010). Numerical analysis for statisticians. Springer Science & Business Media. 

- EM 알고리즘을 이해하는데는 많은 센스가 필요하다. 일단 complete data $X$ 가 무엇인지 모델링을 하는 것 자체가 통계학자의 직관에 많이 좌우한다. 랭은 "***The definition of $X$ is left up to intuition and cleverness of the statistician.***" 이라고 표현하였다. 

- 모델링 된 이후에도 MLE 를 계산해야 하는데 원래 MLE 에 대한 계산자체가 더러운 경우가 많기 때문에 (pdf 자체가 더러운 형태이고 일반 미분해야하는 거면 계산이 힘들다) 통계적 센스와 통찰이 있다면 편하다. 여기에서 통계적 센스와 통찰이 필요하다는 것은 대강의 수식들을 **해석가능한 형태로 유추하며 풀어나가야 한다**는 의미이다. 



### EM 알고리즘은? 

- 우선 현재까지 내가 이해하기로는 EM 알고리즘은 래이턴트-베리어블이 있을 경우 MLE 를 구하는 문제에 쓰인다. 즉 통계학에서 흔히 말하는 **미싱-프라블럼**(missing problem)을 푸는데 사용된다. 사실 여기에만 쓰이는게 아니고 다양하게 쓰일것 같은데 현재까지 내가 찾아본바로는 여기에만 쓰이는것 같다.

- **미싱-프라블럼** 이란 우리가 사실 ${\bf X}$를 관측하고 싶었는데 $t({\bf X})={\bf Z}$를 관측한 상황을 의미한다. 여기에서 $t$ 는 complete data ${\bf X}$를 ${\bf Z}$로 collapses 시키는 어떠한 함수이다. 만약에 
\begin{align}
{\bf X}=({\bf Z},{\bf W})
\end{align}
와 같이 표현가능하다면 $t$ 가 ***simple projection*** 인 특수한 경우라 볼 수 있다. 

- 랜덤벡터 $\bf (Z,W)$ 를 생성하는 어떠한 모수를 $\boldsymbol \theta$ 라고 하자. 우리가 관심있는 것은 $\boldsymbol \theta$ 의 우도함수를 $\boldsymbol \theta$ 를 추론하는 것이다. 그런데 우리는 ${\bf W}$를 관측하지 못했으니까 아래를 구할 수 없다. 
\begin{align}
L({\boldsymbol \theta}|{\bf z},{\bf w})=f({\bf z},{\bf w} | {\boldsymbol\theta})
\end{align}

- 사실 MLE라는 것은 우도함수를 최대화 하는 estimator를 의미하고 우도함수라는 것은 ***given data* 가 주어졌을때** 모수 $\theta$ 가 given data 를 얼마나 잘 설명할 수 있느냐에 대한 설득력을 measure 한 것이라고 이해할 수 있다. 그런데 지금 given data 가 불완전하므로 애초에 우도함수 자체를 구할 수 없는 것이다. 

- EM 알고리즘은 이러한 과정에서 우도함수 비슷한 것 $Q(\boldsymbol{\theta}\|\boldsymbol{\theta}^{(k)})$ 을 정의하고 
\begin{align}
\big(Q(\boldsymbol{\theta}\|\boldsymbol{\theta}^{(k)}),\boldsymbol{\hat \theta}\big)
\end{align} 
을 동시에 업데이트하는 알고리즘이다. 

- 이때 $Q(\boldsymbol{\theta}\|\boldsymbol{\theta}^{(k)})$ 는 ***quasi-likelihood*** 의 $q$ 에서 기호를 따온것이 아닐까 하는 생각이 든다. 

--- 

### EXAMPLE: Mixture model 

- 이번 예제에서는 예제를 푸는것과 동시에 EM-알고리즘에 대한 대략적인 설명을 곁들인다. 

- 아래와 같은 자료를 고려하자. 
\begin{align}
Z_i=W_iX_i + (1-W_i)Y_i, \quad i=1,\dots,n 
\end{align}
아래를 가정하자. <br/><br/>
**(1)** 모든 관측치는 서로 독립이다. <br/>
**(2)** $W_i$ 는 확률이 $p$인 베르누이 분포에서 생성되었다. <br/>
**(3)** $X_i$ 와 $Y_i$ 는 각각 평균이 $\lambda_1$, $\lambda_2$ 인 포아송분포에서 생성되었다. 

- 관측된 자료는 $\bf z$ 이고 관측하고 싶었던 자료, 즉 완전한 자료는 $\bf (z,w)$ 이다. **완전한 자료를 $\bf (x,y,w)$ 로 두지 않음을 유의하자.** 

- 물론 $\bf (x,y,w)$ 가 완전한 자료 맞다. 하지만 완전한 자료를 $\bf (x,y,w)$ 로 두면 우리가 관측한 자료 $\bf z$를 하나도 이용할 수 없다. 완전한 자료를 $\bf (x,y,w)$ 로 두면 통계적센스가 절망적인 사람이다. (나처럼) 

- 우리가 알고 싶은 것은 $(\lambda_1,\lambda_2,p)$ 이므로 MLE를 구하기 위해서 $(\lambda_1,\lambda_2,p)$ 의 우도함수 $L(\lambda_1,\lambda_2,p)$ 를 생각하자. 그런데 **자료가 모두 관측되었다고 가정하면** $(\lambda_1,\lambda_2,p)$ 의 우도함수는 $\bf(z,w)$의 **조인트-pdf** 이므로 아래와 같이 쓸 수 있다. 
\begin{align}
L & (\lambda_1,\lambda_2,p ~\|~ {\bf z},{\bf w}) \\\\ \\
& =f({\bf z,w}\|\lambda_1,\lambda_2,p)=f({\bf w}\|p) f({\bf z} \| {\bf w}; \lambda_1,\lambda_2) \\\\ 
& =\prod_{i=1}^{n}f(w_i \| p)f(z_i|w_i; \lambda_1,\lambda_2)
\end{align}
이제 $f(w_i \| p)$ 와 $f(z_i \| w_i ; \lambda_1,\lambda_2)$ 를 구해야 한다. 각각은 아래와 같이 풀 수 있다. <br/><br/>
**(1)** $f(w_i\|p)=p^{w_i}+(1-p)^{1-w_i}$ <br/>
**(2)** $f(z_i\|w_i ; \lambda_1,\lambda_2)=
\begin{cases} 
\frac{e^{-\lambda_1} \lambda_1^{z_i}}{z_i !} & w_i=1 \\\\ 
\frac{e^{-\lambda_2} \lambda_2^{z_i}}{z_i !} & w_i=0
\end{cases}=\left(\frac{e^{-\lambda_1} \lambda_1^{z_i}}{z_i !} \right)^{w_i} \left(\frac{e^{-\lambda_2} \lambda_2^{z_i}}{z_i !} \right)^{1-w_i}$ <br/><br/> 
여기에서 $f(z_i\|w_i; \lambda_1,\lambda_2)$ 는 다소 직관적으로 구했다. **~~원래 믹스처모델은 대충해도 된다.~~**
아무튼 이걸 정리하면 아래와 같이 된다. 
\begin{align}
L(\lambda_1,\lambda_2,p ~\|~ {\bf z},{\bf w})=\prod_{i=1}^{n}\left(\frac{pe^{-\lambda_1} \lambda_1^{z_i}}{z_i !} \right)^{w_i} \left(\frac{(1-p)e^{-\lambda_2} \lambda_2^{z_i}}{z_i !} \right)^{1-w_i}
\end{align}

- 사실 완전한 자료 $\bf (z,w)$ 를 관측했다면 $L(\lambda_1,\lambda_2,p ~\|~ {\bf z},{\bf w})$ 를 최소화하는 $(\lambda_1,\lambda_2,p)$를 구하면 끝난다. 즉 아래를 연립해서 풀면 끝난다. (아, 물론 정석적으로 하려면 $L(\lambda_1,\lambda_2,p)$ 이 ***covex*** 임도 보여야 함.. 그니까 미분을 총 9번해야함..) 
\begin{align}
\begin{cases}
\frac{\partial}{\partial \lambda_1} L(\lambda_1,\lambda_2,p ~\|~ {\bf z},{\bf w})= 0 \\\
\frac{\partial}{\partial \lambda_2} L(\lambda_1,\lambda_2,p ~\|~ {\bf z},{\bf w}) = 0 \\\
\frac{\partial}{\partial p} L(\lambda_1,\lambda_2,p ~\|~ {\bf z},{\bf w}) =0 
\end{cases}
\end{align}

- 그런데 골치 아픈 것은 우리가 관측한 자료가 사실 $\bf (z,w)$가 아니라 $\bf z$ 라는 것이다. 그래서 $L(\lambda_1,\lambda_2,p ~\|~ {\bf Z}={\bf z},{\bf W}={\bf w})$ 는 우도함수가 아니라 **랜덤변수** 가 된다. 즉 우리가 그동안 우도함수라고 생각하고 구해왔던 것이 아래와 같은 모양이 되어버린다. 
\begin{align}
\prod_{i=1}^{n}\left(\frac{pe^{-\lambda_1} \lambda_1^{z_i}}{z_i !} \right)^{W_i\|z_i} \left(\frac{(1-p)e^{-\lambda_2} \lambda_2^{z_i}}{z_i !} \right)^{1-W_i\|z_i}
\end{align}
위의 식은 $W_i\|z_i$가 포함되어있으므로 랜덤변수이다. 따라서 이것을 미분하여 최적값을 구할 순 없다. (최적화 하고 싶은 함수가 랜덤하게 값이 바뀌는데 거기서 그냥 그레디언트류의 방법들을 적용하면 **랜덤최적값(?)** 같은게 구해질텐데 그걸 구하고 있을 순 없잖음??) 

- EM에서 가장 어려운 부분은 바로 
\begin{align}
\prod_{i=1}^{n}\left(\frac{pe^{-\lambda_1} \lambda_1^{z_i}}{z_i !} \right)^{W_i\|z_i} \left(\frac{(1-p)e^{-\lambda_2} \lambda_2^{z_i}}{z_i !} \right)^{1-W_i\|z_i}
\end{align}
어떤 노테이션으로 쓸지 애매하다는 것이다. 일반적으로 표현하는 방법이 있는데 이것은 후술하고 일단 나는 이것을 아래와 같이 정의하겠다. 
\begin{align}
{\bf L}^* := {\bf L}^* ({\bf W}\|{\bf z} ,\lambda_1,\lambda_2,p) = \prod_{i=1}^{n}\left(\frac{pe^{-\lambda_1} \lambda_1^{z_i}}{z_i !} \right)^{W_i\|z_i} \left(\frac{(1-p)e^{-\lambda_2} \lambda_2^{z_i}}{z_i !} \right)^{1-W_i\|z_i}
\end{align} 
기호 ${\bf L}^* $ 의 우수성은 다음과 같다.  (1) 이것이 ${\bf W}\|{\bf z}$에 대한 함수임을 밝혔는데 이것은 ${\bf L}^* $ 이 랜덤임을 의미함과 동시에 ${\bf z}$ 가 given 되어야 구할 수 있음을 의미한다. (2) 또한  ${\bf L}^* $ 이 모수  $\lambda_1,\lambda_2,p$ 에 대한 함수임도 밝혔는데 이것은 뒤에 E-step, M-step 을 정의할때 매우 명확한 노테이션을 제공한다. 

- 아무튼 ${\bf L}^* ({\bf W}\|{\bf z} ,\lambda_1,\lambda_2,p) $를 바로 최적화할 수 는 없지만 (랜덤최적값 나옴) 평균을 구한뒤에 최적값을 구하면 될 것 **같다**. 즉 아래를 계산하면 될 것 **같다.**
\begin{align}
\mathbb{E}_ {\boldsymbol \theta} \log {\bf L}^* ({\bf W}\|{\bf z} ,\lambda_1,\lambda_2,p) = \mathbb{E}_ {\boldsymbol \theta}{\boldsymbol\ell}^* ({\bf W}\|{\bf z} ,\lambda_1,\lambda_2,p)
\end{align}
하지만 여기에서 '어, 그래 맞네. 이제 $\mathbb{E}_ {\boldsymbol \theta} {\boldsymbol\ell}^* ({\bf W}\|{\bf z} ,\lambda_1,\lambda_2,p)$ 만 구하면 되겠네 다 끝났네' 라고 생각하는 사람은 통계센스가 절망적인 사람이다. (나처럼..) 왜냐하면
\begin{align}
\mathbb{E}_ {\boldsymbol \theta} {\boldsymbol\ell}^* = \mathbb{E}_ {\boldsymbol \theta} \bigg[ \sum_{i=1}^{n}W_i\|z_i \log \Big(\frac{pe^{-\lambda_1} \lambda_1^{z_i}}{z_i !} \Big) + (1-W_i\|z_i)\log \Big(\frac{(1-p)e^{-\lambda_2} \lambda_2^{z_i}}{z_i !}\Big) \bigg]
\end{align}
를 계산하려면 $W_i \| z_i$ 의 분포를 알아야 하고 이걸 알기 위해서는 $ {\boldsymbol \theta}$ 를 알아야 하는데, 우리가 지금 하고 있는게 사실 $  {\boldsymbol \theta}$를 알기 위한 것임을 생각해보면 이 과정에서 **순환논리의 모순**이 생기기 때문이다. (**A**:너 어디가? **B**: 너 따라가고 있었어 **A**: 나는 너 따라가고 있었는데?) 

- 이걸 해결한것이 바로 EM 이다. 아이디어는 진짜 간단하다. $\mathbb{E}_ {\boldsymbol{\theta}}{\boldsymbol\ell}^* $ 를 계산 못하겠으니까 
$\mathbb{E}_ {\boldsymbol{\theta}^{(k)}}{\boldsymbol\ell}^* $ 를 대신 계산하고 ***iteration*** 을 통하여 근사시키겠다는 것이다. EM 알고리즘은 아래와 같이 2가지 스텝을 반복하여 풀것을 제안한다. <br/><br/> 
**[$E$-step]**  $\quad~$ 아래와 같이 ***quasi-likelihood*** 를 정의하고 그것을 계산함. 
\begin{align}
Q(\boldsymbol{\theta}\|\boldsymbol{\theta}^{(k)}):=\mathbb{E}_ {\boldsymbol{\theta}^{(k)}}{\boldsymbol\ell}^* \big({\bf W}\|{\bf z} ,\lambda_1,\lambda_2,p\big)
\end{align}
**[$M$-step]** $\quad$ ***quasi-likelihood*** 를 최대화하는 $\boldsymbol\theta$ 를 찾고 그 값을 $\boldsymbol\theta^{(k+1)}$ 로 ***update*** 함. 

- **($E$-step)** 이제부터는 그냥 단순계산만 하면된다. 
\begin{align}
\quad Q(\boldsymbol{\theta}\|\boldsymbol{\theta})&=\mathbb{E}_ {\boldsymbol{\theta}^{(k)}} {\boldsymbol\ell}^* \big({\bf W}\|{\bf z} ,\lambda_1,\lambda_2,p\big) \\\\ \\\\
&=\mathbb{E}_ {\boldsymbol{\theta}^{(k)}} \Bigg[ \sum_{i=1}^{n}\Big(W_i\|z_i \log\big(\mbox{blah-blah}\big)+(1-W_i\|z_i)\log\big(\mbox{yadi-yadi}\big) \Big)\Bigg]
\end{align}
여기에서 **블라블라*(blah-blah)*** 는 
\begin{align}
\frac{pe^{-\lambda_1} \big(\lambda_1\big)^{z_i}}{z_i !}
\end{align}
이고 **야디야디*(yadi-yadi)*** 는 
\begin{align}
\frac{(1-p)e^{-\lambda_2} \big(\lambda_2\big)^{z_i}}{z_i !}
\end{align}
이다. 둘다 "***funtion of*** $\boldsymbol{\theta}$" 이므로 블라블라와 야디야디는 $M$-step 에서 실제로 미분이 수행되는 중요한 부분이다. 이제 남은건 $\mathbb{E}_ {\boldsymbol{\theta}^{(k)}}(W_i|z_i)$ 이다. 이는 아래와 같이 구한다. ~~(그냥 때려맞추면 됨)~~
\begin{align}
\mathbb{E}_ {\boldsymbol{\theta}^{(k)}}(W_i|z_i)=\frac{p^{(k)}e^{-\lambda_1^{(k)}}\big(\lambda_1^{(k)}\big)^{z_i}}{p^{(k)}e^{-\lambda_1^{(k)}}\big(\lambda_1^{(k)}\big)^{z_i}+(1-p^{(k)})e^{-\lambda_2^{(k)}}\big(\lambda_2^{(k)}\big)^{z_i}}
\end{align}

- **($M$-step)** 이제 quasi-likelihood 를 미분하면 된다. 이때 ${\boldsymbol\theta}^{(k)}$는 상수취급하고 나머지 ${\boldsymbol\theta}$ 로 표기된 부분만 미분하면 되어서 그나마 계산하기 편하다. 
\begin{align}
\begin{cases}
\frac{\partial}{\partial \lambda_1}Q(\boldsymbol{\theta}\|\boldsymbol{\theta}^{(k)}) = 0 \\\\ \\
\frac{\partial}{\partial \lambda_2}Q(\boldsymbol{\theta}\|\boldsymbol{\theta}^{(k)}) = 0 \\\\ \\
\frac{\partial}{\partial p}Q(\boldsymbol{\theta}\|\boldsymbol{\theta}^{(k)}) =0
\end{cases}
\end{align}
이제 위의 식을 연립하여 푼 해를 $\boldsymbol{\theta}^{(k+1)}$ 에 대입한뒤 $E$-step 을 반복한다. 

--- 

- 마지막으로 노테이션에 대한 언급을 하면서 마무리하겠다. 나는 계속 위에서 $W_i\|z_i$ 와 같은것을 마치 확률변수인것 처럼 취급하였다. 하지만 보통 이런식으로 취급하지 않는다. 그래서 
\begin{align}
{\bf L}^* ({\bf W}\|{\bf z} ,\lambda_1,\lambda_2,p)
\end{align}
와 같은 기호도 일반적인것은 아니다. 앞에서 밝혔듯이 개인적으로 이걸 적절한 기호로 정의하는게 까다롭다고 생각한다. (내 노테이이션이 젤 좋다) 그래서 사람마다 콰시-라이클리후드를 표현하는 노테이션이 조금씩 미묘하게 다르다. 일단 임요한 교수님 강의노트에는 
\begin{align}
Q(\boldsymbol{\theta}\|\boldsymbol{\theta}^{(k)})=\mathbb{E}_ {\boldsymbol{\theta}^{(k)}} \Big[ l({\boldsymbol \theta} \|{\bf z},{\bf w}) \Big\| {\bf z} \Big]
\end{align}
와 같이 정의한다. 랭의 교재에서는 아래와 같이 표현하였다. 여기에서 ${\bf X}$는 ***complete data*** 이다. 
\begin{align}
Q(\boldsymbol{\theta}\|\boldsymbol{\theta}^{(k)})=\mathbb{E}\Big[ \log f({\bf X}\|{\boldsymbol\theta}) \Big\| {\bf z},\boldsymbol{\theta}^{(k)}\Big]
\end{align}
솔직히 둘 다 개인적으로 별로여서 (아마 저 notation 들을 쓰다보면 단점이 보일것이다) 나는 
\begin{align}
Q(\boldsymbol{\theta}\|\boldsymbol{\theta}^{(k)}):=\mathbb{E}_ {\boldsymbol{\theta}^{(k)}} {\boldsymbol\ell}^* \big({\bf W}\|{\bf z} ,\lambda_1,\lambda_2,p\big)
\end{align}
와 같이 정의하는게 젤 좋은것 같다. 
